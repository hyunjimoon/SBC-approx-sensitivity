---
title: "SelfConsistentPrior"
output: html_document
---

```{r setup, include=FALSE}
library(cmdstanr)
library(dplyr)
library(tidyverse)
library(reshape)
library(parallel)
library(posterior)
#library(rstanarm)
library(SBC)
library(pracma)
library("entropy")
devtools::install_github("rmcelreath/rethinking",ref="Experimental")
#library(rethinking)
set.seed(1954)
.libPaths("~/Rlib")
options(mc.cores = parallel::detectCores())
set_get_Dir <- function(modelName){
  scriptDir <- getwd()
  modDir <- file.path(scriptDir, "models")
  dataDir <- file.path(scriptDir, "data")
  delivDir <- file.path(scriptDir, "deliv", modelName)
  dir.create(delivDir)
  file <- file.path(modDir, modelName, paste0(modelName, ".stan"))
  mod <- cmdstan_model(file)
  return(list(data = data, mod = mod, modDir = modDir, file = file, delivDir = delivDir)) 
}

```

# Normal-Normal conjugate, unkown mean (theta) and known sd(sigma)

$$
\theta \sim N(\mu, s^2)\\
y \sim N(\theta, \sigma^2)\\
\theta|y \sim N(\frac{s^2}{s^2 +\sigma^2} *\bar{y}+ \frac{\sigma^2}{s^2 +\sigma^2} * \mu, \frac{s^2 * \sigma^2}{s^2 +  \sigma^2})\\
\sigma = 1, s = 0.1 \;\text{or} \; 5
$$
# 1. Designing iteration
```{R}
source("tools/selfCalib.R")
modelName = "simple_normal"
modDir <- set_get_Dir(modelName)$modDir
delivDir <- set_get_Dir(modelName)$delivDir
file <- set_get_Dir(modelName)$file
simple_normal = cmdstanr::cmdstan_model(file)
generator <- function(){  
    function(){
    theta <- rnorm(1, 0, 1)
    list(
      generated = rnorm(8, theta, 1),
      parameters = list(
        theta = theta
      )
    )
  }
}
D <- 8
y <- rep(0, D) # placeholder
data = list("D"= D, "y"= y, "theta_loc"= 0, "theta_scale"= 1)
nChains <- 4
parallel_chains <- min(nChains, detectCores())
N = 20 #200
M = 5 # 40
workflow <- SBC::SBCWorkflow$new(simple_normal, generator())
workflow$simulate(N) 
workflow$fit_model(sample_iterations = M, warmup_iterations = M, data)
prior <- workflow$prior_samples
post <- workflow$posterior_samples
workflow$calculate_rank()

#target calibration
pars<-  "theta" #names(prior)[c(1)] 
evolve_df <- initDf(1, summary = "pars")
#sc_workflow <- SBCWorkflow$new(simple_normal, generator())
sc_prior <-  selfCalib(stan_model = simple_normal, prior, pars, data, N, M, cnt = 1, evolve_df, delivDir, is_param = FALSE)
# test self-consistency fo sc_prior
sc_workflow$simulate(n_sbc_iterations = N, custom_prior = sc_prior)
#clamped stan could be applied for inference
sc_workflow$fit_model(sample_iterations = M, warmup_iterations = M, data)
sc_ranks <- sc_workflow$calculate_rank()
plot_ecdf(as_draws_matrix(sc_ranks), "mu")
plot_hist(as_draws_matrix(sc_ranks), "mu", bins = 10)
print_summary(as_draws_matrix(sc_ranks), "mu", bins = 10)
# parameteric generator and stan prior
par <- c("theta")

```
# 1. The effect of parameter scale: s 5, .1, 100
Prevent converging to the point mass; entropy term is needed.
## 1-1. Normal prior (N) + Normal data simulator (N) + conjugate perfect posterior simulator (NM)/ Plot mean, sd
```{R}
sigma <- 1
sigma2 <- sigma * sigma
I <- 2000 # Number of iterations
N <- 1000 # Number of prior samples (= total data samples, one dataset for each prior)
M <- 25 # Number of posterior samples
m <- 0
s0 <-5
s <- s0
s2 <- s * s
df <- initDf(I, summary = "ms")
for (i in 1:I) {
  # prior simulator
  theta <- rnorm(N, m, s) # N prior samples, laplace approximation of posterior samples
  # data simulator
  y <- rnorm(N, theta, sigma) # one data sample from each prior
  # posterior simulator
  post_m <- (y * s2 + m * sigma2) / (s2 + sigma2) # M posterior samples from each data
  post_s <- sqrt( s2 * sigma2 / (s2 + sigma2) )
  post <- matrix(nrow = N, ncol = M)
  for (n in 1:N){
    post[n,]<- rnorm(M, post_m[n], post_s)
  }
  post_theta <- post_summ(post, sumtype = "Identity")
  m  <- mean(post_theta)
  s  <- sd(post_theta)
  s2 <- s * s
  df$mean[i] <- m
  df$sd[i] <- s
}
e_df <- melt(evolve_df, id.vars = 'iter')
e_df <- melt(e_df, id.vars = 'iter')
df <- melt(df, id.vars = 'iter')
ggplot(df, aes(x = iter, y = value,  color = variable) ) +
  geom_point() + ggtitle(sprintf("priorSd: %s, N: %s, M: %s ", s0, N, M))
```
Convergence is easier with smaller s as the impact of data is smaller.

## 1-1. Normal prior (N) + Normal data simulator (N) + conjugate perfect posterior simulator (NM)/ Plot mean, sd
```{R}
sigma <- 1
sigma2 <- sigma * sigma
I <- 20000 # Number of iterations
N <- 1000 # Number of prior samples (= total data samples, one dataset for each prior)
M <- 25 # Number of posterior samples
m <- 0
s0 <-.1
s <- s0
s2 <- s * s
df <- initDf(I, summary = "q")
for (i in 1:I) {
  # prior simulator
  theta <- rnorm(N, m, s) # N prior samples, laplace approximation of posterior samples
  # data simulator
  y <- rnorm(N, theta, sigma) # one data sample from each prior
  # posterior simulator
  post_m <- (y * s2 + m * sigma2) / (s2 + sigma2) # M posterior samples from each data
  post_s <- sqrt( s2 * sigma2 / (s2 + sigma2) )
  post <- matrix(nrow = N, ncol = M)
  for (n in 1:N){
    post[n,]<- rnorm(M, post_m[n], post_s)
  }
  post_theta <- post_summ(post, sumtype = "Identity")
  m  <- mean(post_theta)
  s  <- sd(post_theta)
  s2 <- s * s
  q <-  quantile(post_theta, c(.25,.75))
  df$q1[i] <-q["25%"]
  df$q3[i] <- q["75%"]
}
df <- melt(df, id.vars = 'iter')
ggplot(df, aes(x = iter, y = value,  color = variable) ) +
  geom_point() + ggtitle(sprintf("priorSd: %s, N: %s, M: %s ", s0, N, M))
```
Does not converge.


## 1-1. Normal prior (N) + Normal data simulator (N) + conjugate perfect posterior simulator (NM)/ Plot mean, sd
```{R}
I <- 2000 # Number of iterations
N <- 1000 # Number of prior samples (= total data samples, one dataset for each prior)
M <- 25 # Number of posterior samples
m <- 0
s0 <-100
s <- s0
s2 <- s * s
df <- initDf(I, summary = "q")
for (i in 1:I) {
  # prior simulator
  theta <- rnorm(N, m, s) # N prior samples, laplace approximation of posterior samples
  # data simulator
  y <- rnorm(N, theta, sigma) # one data sample from each prior
  # posterior simulator
  post_m <- (y * s2 + m * sigma2) / (s2 + sigma2) # M posterior samples from each data
  post_s <- sqrt( s2 * sigma2 / (s2 + sigma2) )
  post <- matrix(nrow = N, ncol = M)
  for (n in 1:N){
    post[n,]<- rnorm(M, post_m[n], post_s)
  }
  post_theta <- post_summ(post, sumtype = "Identity")
  m  <- mean(post_theta)
  s  <- sd(post_theta)
  s2 <- s * s
  q <-  quantile(post_theta, c(.25,.75))
  df$q1[i] <-q["25%"]
  df$q3[i] <- q["75%"]
}
df <- melt(df, id.vars = 'iter')
ggplot(df, aes(x = iter, y = value,  color = variable) ) +
  geom_point() + ggtitle(sprintf("priorSd: %s, N: %s, M: %s ", s0, N, M))
```
Does not converge. Compared to s = .1 where the breadth of the distribution was maintained, it is becoming narrower.

1-3. s = 1000
```{R}
I <- 20 # Number of iterations
N <- 10 # Number of prior samples (= total data samples, one dataset for each prior)
M <- 25 # Number of posterior samples
m <- 0
s0 <-1000
s <- s0
s2 <- s * s
wsh <- 1000
df <- initDf(I, summary = "q")
for (i in 1:I) {
  # prior simulator
  theta <- rnorm(N, m, s) # N prior samples, laplace approximation of posterior samples
  # data simulator
  y <- rnorm(N, theta, sigma) # one data sample from each prior
  # posterior simulator
  post_m <- (y * s2 + m * sigma2) / (s2 + sigma2) # M posterior samples from each data
  post_s <- sqrt( s2 * sigma2 / (s2 + sigma2) )
  post <- matrix(nrow = N, ncol = M)
  for (n in 1:N){
    post[n,]<- rnorm(M, post_m[n], post_s)
  }
  wsc <- ws(theta, c(post), minBin = -1000, maxBin = 1000, numBins = 100)
  print(paste0("wsh:", wsh))
  print(paste0("wsc:", wsc))
  post_theta <- post_summ(post, sumtype = "Identity")
  m  <- mean(post_theta)
  s  <- sd(post_theta)
  s2 <- s * s
  q <-  quantile(post_theta, c(.25,.75))
  df$q1[i] <-q["25%"]
  df$q3[i] <- q["75%"]
  df$ws[i] <- wsc
  if(wsc < wsh){wsh <- wsc}
}
df <- melt(df, id.vars = 'iter')
df2 <- subset(df, variable %in% c("q1","q3"))
df3 <- subset(df, variable == "ws")
ggplot(df2, aes(x = iter, y = value,  color = variable) ) +
  geom_point() + ggtitle(sprintf("priorSd: %s, N: %s, M: %s ", s0, N, M))
ggplot(df3, aes(x = iter, y = value,  color = variable) ) +
  geom_line()
```
1-3. s = 1000 one more time
```{R}
sigma <- 1
sigma2 <- sigma * sigma
I <- 2000 # Number of iterations
N <- 1000 # Number of prior samples (= total data samples, one dataset for each prior)
M <- 25 # Number of posterior samples
m <- 0
s0 <-.1
s <- s0
s2 <- s * s
df <- initDf(I, summary = "q")
for (i in 1:I) {
  # prior simulator
  theta <- rnorm(N, m, s) # N prior samples, laplace approximation of posterior samples
  # data simulator
  y <- rnorm(N, theta, sigma) # one data sample from each prior
  # posterior simulator
  post_m <- (y * s2 + m * sigma2) / (s2 + sigma2) # M posterior samples from each data
  post_s <- sqrt( s2 * sigma2 / (s2 + sigma2) )
  post <- matrix(nrow = N, ncol = M)
  for (n in 1:N){
    post[n,]<- rnorm(M, post_m[n], post_s)
  }
  wsc <- ws(theta, c(post))
  print(paste0("wsh:", wsh))
  print(paste0("wsc:", wsc))
  post_theta <- post_summ(post, sumtype = "Identity")
  m  <- mean(post_theta)
  s  <- sd(post_theta)
  s2 <- s * s
  q <-  quantile(post_theta, c(.25,.75))
  df$q1[i] <-q["25%"]
  df$q3[i] <- q["75%"]
  df$ws[i] <- wsc
  if(wsc < wsh){wsh <- wsc}
}
df <- melt(df, id.vars = 'iter')
df2 <- subset(df, variable %in% c("q1","q3"))
df3 <- subset(df, variable == "ws")
ggplot(df2, aes(x = iter, y = value,  color = variable) ) +
  geom_point() + ggtitle(sprintf("priorSd: %s, N: %s, M: %s ", s0, N, M))
ggplot(df3, aes(x = iter, y = value,  color = variable) ) +
  geom_line()
```
-  Converge, but to a point mass which has limited use as a self-consistent measure. Also, this point mass is not unique as repeated experiment converged to different point mass. Another force that pushes sd away from 0 is needed; entropy.
- Smaller N and higher s (increased data dependence), converge to a point mass faster.

s = .1, N = 10000 will be used for the following experiment.

#2. The effect of summarizing posterior
## 2-1. Normal prior (N) + Normal data simulator (N) + conjugate perfect posterior simulator (NM)/ No summary/ Plot quantile
```{R}
I <- 2000 # Number of iterations
N <-  1000# Number of prior samples (= total data samples, one dataset for each prior)
M <- 25 # Number of posterior samples
E <- 3 # Number of experiment replication
df <- initDf(E * I, summary = "q")
for (e in 1:E){
  m <- 0
  s0 <-.1
  s <- s0
  s2 <- s * s
  for (i in 1:I) {
    # prior simulator
    theta <- rnorm(N, m, s) # N prior samples, laplace approximation of posterior samples
    # data simulator
    y <- rnorm(N, theta, sigma) # one data sample from each prior
    # posterior simulator
    post_m <- (y * s2 + m * sigma2) / (s2 + sigma2) # M posterior samples from each data
    post_s <- sqrt( s2 * sigma2 / (s2 + sigma2) )
    post <- matrix(nrow = N, ncol = M)
    for (n in 1:N){
      post[n,]<- rnorm(M, post_m[n], post_s)
    }
    wsc <- ws(theta, c(post))
    if(wsc < wsh){wsh <- wsc}
    print(paste0("wsh:", wsh))
    print(paste0("wsc:", wsc))
    post_theta <- post_summ(post, sumtype = "Identity")
    m  <- mean(post_theta)
    s  <- sd(post_theta)
    s2 <- s * s
    q <-  quantile(post_theta, c(.25,.75))
    df$q1[(e-1)*I + i] <-q["25%"]
    df$q3[(e-1)*I + i] <- q["75%"]
    df$ws[(e-1)*I + i] <- wsc
    df$e[(e-1)*I + i] <- e
    df$i[(e-1)*I + i] <- i
  }
}
idx <- c('iter','e', 'i')
df1 <- melt(df, id.vars = idx) #TODO log scale
ggplot(df1, aes(x = i, y = value,  color = variable) ) +
geom_point() + ggtitle(sprintf("priorSd: %s, N: %s, M: %s ", s0, N, M)) + 
facet_wrap(~e)
# df2 <- melt(df, id.vars = 'iter')
# df3 <- subset(df2, variable == "ws")
# ggplot(df3, aes(x = iter, y = value,  color = variable) ) +
#   geom_line() + facet_wrap(~e)    
```
Does not converge to a unique measure.

## 2-2. Imperfect posterior sampler: 1.Normal prior (N) + 2.Normal data simulator (N) + 3.[conjugate perfect posterior simulator (NM) + summarized posterior (N from NM)]
```{r}
I <- 2000 # Number of iterations
N <-  1000# Number of prior samples (= total data samples, one dataset for each prior)
M <- 25 # Number of posterior samples
E <- 3 # Number of experiment replication
df <- initDf(E * I, summary = "q")
for (e in 1:E){
  m <- 0
  s0 <-.1
  s <- s0
  s2 <- s * s
  for (i in 1:I) {
    # prior simulator
    theta <- rnorm(N, m, s) # N prior samples, laplace approximation of posterior samples
    # data simulator
    y <- rnorm(N, theta, sigma) # one data sample from each prior
    # posterior simulator
    post_m <- (y * s2 + m * sigma2) / (s2 + sigma2) # M posterior samples from each data
    post_s <- sqrt( s2 * sigma2 / (s2 + sigma2) )
    post <- matrix(nrow = N, ncol = M)
    for (n in 1:N){
      post[n,]<- rnorm(M, post_m[n], post_s)
    }
    post_theta <- post_summ(post, sumtype = "resample")
    wsc <- ws(theta, post_theta)
    if(wsc < wsh){wsh <- wsc}
    print(paste0("wsh:", wsh))
    print(paste0("wsc:", wsc))
    m  <- mean(post_theta)
    s  <- sd(post_theta)
    s2 <- s * s
    q <-  quantile(post_theta, c(.25,.75))
    df$q1[(e-1)*I + i] <-q["25%"]
    df$q3[(e-1)*I + i] <- q["75%"]
    df$ws[(e-1)*I + i] <- wsc
    df$e[(e-1)*I + i] <- e
    df$i[(e-1)*I + i] <- i
  }
}
idx <- c('iter','e', 'i')
df1 <- melt(df, id.vars = idx)
ggplot(df1, aes(x = i, y = value,  color = variable) ) +
geom_point() + ggtitle(sprintf("priorSd: %s, N: %s, M: %s ", s0, N, M)) + 
facet_wrap(~e)
```
Converge to a unique measure.

1-3. Normal prior (N) + Normal data simulator (N) + conjugate perfect posterior simulator (NM) + No summary/ Plot quantile
```{r}
I <- 2000
N <- 1000 # Number of prior samples (= total data samples, one dataset for each prior)
M <- 25 # Number of posterior samples
m <- 0
s0 <-.1
s <- s0
s2 <- s * s
df <- initDf(I, summary = "q")
for (i in 1:I) {
  # prior simulator
  theta <- rnorm(N, m, s) # N prior samples
  # data simulator
  y <- rnorm(N, theta, sigma) 
  # posterior simulator
  post_m <- (y * s2 + m * sigma2) / (s2 + sigma2) # M posterior samples from each data
  post_s <- sqrt( s2 * sigma2 / (s2 + sigma2) )
  post <- matrix(nrow = N, ncol = M)
  for (n in 1:N){
    post[n,]<- rnorm(M, post_m[n], post_s)
  }
  post_theta <- post_summ(post, sumtype = "Identity")
  m  <- mean(post_theta)
  s  <- sd(post_theta)
  s2 <- s * s
  q <-  quantile(post_theta, c(.25,.75))
  df$q1[i] <-q["25%"]
  df$q3[i] <- q["75%"]
}
df <- melt(df, id.vars = 'iter')
ggplot(df, aes(x = iter, y = value,  color = variable) ) +
  geom_point() + ggtitle(sprintf("priorSd: %s, N: %s, M: %s ", s0, N, M))
```

# 3 different data sampler
## 3-1. Gamma prior + Poisson data simulator + perfect posterior simulator (conjugate)/ N prior, N dataset, N summarized posterior from NM / Plot quantile 
```{r approximate posterior simulator}
I <- 2000
N <- 10000 # Number of data samples
M <- 25 # Number of posterior samples
mean <- 10
s <- 1
s2 <- s*s
nu = mean/s2
alpha =nu*mean
df <- initDf(I, summary = "q")
for (i in 1:I) {
  theta <- rgamma(N, shape=alpha, rate=nu)
  # data simulator
  y <- rpois(N, theta)
  # posterior simulator
  post_alpha <- alpha + sum(y)
  post_nu <- nu + length(y)
  post_theta <- rgamma(M, post_alpha, post_nu)
  # summarize posterior for iteration
  mean <- mean(post_theta) # laplace approximation
  s <- sd(post_theta)
  s2 <- s*s
  nu <- mean/s2
  alpha <- nu*mean
  post <- matrix(nrow = N, ncol = M)
  for (n in 1:N){
    post[n,]<- rnorm(M, post_m[n], post_s)
  }
  post_theta <- post_summ(post, sumtype = "Identity")
  q <-  quantile(post_theta, c(.25,.75))
  df$q1[i] <-q["25%"]
  df$q3[i] <- q["75%"]
}
df <- melt(df, id.vars = 'iter')
ggplot(df, aes(x = iter, y = value,  color = variable) ) +
  geom_point() + ggtitle(sprintf("%s data %s posterior", N, M))
```
## 3-2. Gamma prior (N) + Poisson data simulator (N) + Laplace approximate posterior simulator  (NM) + posterior summary (N from NM)/ Plot quantile 
```{r approximate posterior simulator}
I <- 5000
N <- 10000 # Number of data samples
M <- 25 # Number of posterior samples
mean <- 10
s <- 1
s2 <- s*s
nu = mean/s2
alpha =nu*mean
df <- initDf(I, summary = "q")
for (i in 1:I) {
  theta <- rgamma(N, shape=alpha, rate=nu)
  # data simulator
  y <- rpois(N, theta)
  # posterior simulator
  post_alpha <- alpha + sum(y)
  post_nu <- nu + length(y)
  post_theta <- rgamma(M, post_alpha, post_nu)
  # summarize posterior for iteration
  mean <- mean(post_theta) # laplace approximation
  s <- sd(post_theta)
  s2 <- s*s
  nu <- mean/s2
  alpha <- nu*mean
  post <- matrix(nrow = N, ncol = M)
  for (n in 1:N){
    post[n,]<- rnorm(M, post_m[n], post_s)
  }
  post_theta <- post_summ(post, replace = FALSE)
 
  df$q1[i] <- quantile(post_theta, .75) # quantiles are kept for approximated posteriors
  df$q3[i] <- quantile(post_theta, .25)
}
df <- melt(df, id.vars = 'iter', variable.name = 'stat')
ggplot(df, aes(x = iter, y = value,  color = stat) ) +
  geom_point() + ggtitle(sprintf("%s data %s posterior", N, M))
``` 

#4. Gradient of $P_{WC}$

```{r}
sigma <- 1
sigma2 <- sigma * sigma
I <- 2000 # Number of iterations
N <- 1000 # Number of prior samples (= total data samples, one dataset for each prior)
M <- 100 # Number of posterior samples
m <- 0
s0 <-1
s <- s0
s2 <- s * s
df <- initDf(I, summary = "q")
wsh <- 1000000000
for (i in 1:I) {
  # prior simulator
  theta <- rnorm(N, m, s) # N prior samples, laplace approximation of posterior samples
  # data simulator
  y <- rnorm(N, theta, sigma) # one data sample from each prior
  # posterior simulator
  post_m <- (y * s2 + m * sigma2) / (s2 + sigma2) # M posterior samples from each data
  post_s <- sqrt( s2 * sigma2 / (s2 + sigma2) )
  post <- matrix(nrow = N, ncol = M)
  for (n in 1:N){
    post[n,]<- rnorm(M, post_m[n], post_s)
  }
  wsc <- ws(theta, c(post))
  print(paste0("wsh:", wsh))
  print(paste0("wsc:", wsc))
  post_theta <- post_summ(post, sumtype = "Identity")
  m  <- mean(post_theta)
  s  <- sd(post_theta)
  s2 <- s * s
  q <-  quantile(post_theta, c(.25,.75))
  df$q1[i] <-q["25%"]
  df$q3[i] <- q["75%"]
  if(wsc < wsh){wsh <- wsc}
}
df <- melt(df, id.vars = 'iter')
ggplot(df, aes(x = iter, y = value,  color = variable) ) +
  geom_point() + ggtitle(sprintf("priorSd: %s, N: %s, M: %s ", s0, N, M))
```


#5. Linear regression data simulator, posterior simulator
```{r stan included}
source("tools/selfCalib.R")
modelName = "eightschools_ncp"
modDir <- set_get_Dir(modelName)$modDir
delivDir <- set_get_Dir(modelName)$delivDir
file <- set_get_Dir(modelName)$file
ncp_model = cmdstanr::cmdstan_model(file)
generator <- function(){
  function(){
    mu <- rnorm(1, 0, 5)
    tau <- rcauchy(1, 0, 5)
    theta_trans <- rnorm(8, 0, 1)
    theta <-theta_trans * tau + mu
    list(
      generated = rnorm(8, mu, sigma),
      parameters = list(
        mu = mu,
        theta_trans=theta_trans,
        tau = tau,
        theta = theta
      )
    )
  }
}

J <- 8
y <- c(28, 8, -3, 7, -1, 1, 18, 12)
sigma <- c(15, 10, 16, 11, 9, 11, 10, 18)
nChains <- 4
parallel_chains <- min(nChains, detectCores())
N = 100 #200
M = 20 # 40

# data needed for y_sim placeholders
data = list("J"=J, "y"=y, "sigma"=sigma)
workflow <- SBCWorkflow$new(ncp_model, generator())
workflow$simulate(n_sbc_iterations = N) 
workflow$fit_model(sample_iterations = M, warmup_iterations = M, data)
prior <- workflow$prior_samples
post <- workflow$posterior_samples
ranks <- workflow$calculate_rank()
plot_ecdf(as_draws_matrix(ranks), "mu")
plot_hist(as_draws_matrix(ranks), "mu", bins = 10)
print_summary(as_draws_matrix(ranks), "mu", bins = 10)


# expected to be uniform
```

#7. Linear regression
```{r multi regression}
P = 2
modelName = "multireg"
modDir <- set_get_Dir(modelName)$modDir
file <- set_get_Dir(modelName)$file
model_reg = cmdstanr::cmdstan_model(file)
sbc_obj_reg = SBC::SBCModel$new(name="linreg", stan_model=model_reg)
par_names <- list("param")
# dummy data - need to change stan file
genFakeData_multireg <- function(){
  param_loc = rnorm(P, 0, 1)
  param_scale = rgamma(P, 1, 3)
  X = rmvnorm(N*P, mean = c(0, 0), sigma = diag(c(1,1))) 
  y = rnorm(X %*% param, 1.2)
  data = list("N"=N, "X"= X, "y" = y, "param_loc" = param_loc, "param_scale" = param_scale)
  return (data)
}
data <- genFakeData_multireg()
prior_theta_reg = sbc_obj_reg$sample_theta_tilde_stan(par_names, N, data=data)

# self-consistent region where prior/posterior var converge
safe_prio <- iter_gen_inf(modelName, sbc_obj_reg, prior_theta_reg, par_names, N, M, data, cnt =0)
```


## 1.Different priors: same SCS but faster with tighter prior
```{r linear regression}
modelName = "linreg"
modDir <- set_get_Dir(modelName)$modDir
file <- set_get_Dir(modelName)$file
model_reg = cmdstanr::cmdstan_model(file)
sbc_obj_reg = SBC::SBCModel$new(name="linreg", stan_model=model_reg)
par_names <- list("alpha", "beta")
# dummy data - need to change stan file
genFakeData_linreg <- function(){
  a = rnorm(N, 0, 50)
  b = rnorm(N, 0, 50)
  X = rnorm(N, 0, 5)
  y = rnorm(a + b * X, 1.2)
  alpha_loc = mean(a)
  alpha_scale = sd(a)
  beta_loc = mean(b) 
  beta_scale = sd(b)
  data = list("N"=N, "X"= X, "y" = y, "alpha_loc" = alpha_loc, "alpha_scale" = alpha_scale, "beta_loc" = beta_loc, "beta_scale" = beta_scale)
  return (data)
}
data <- genFakeData_linreg()
prior_theta_reg = sbc_obj_reg$sample_theta_tilde_stan(par_names, N, data=data)

# self-consistent region where prior/posterior var converge
safe_prio <- iter_gen_inf(modelName, sbc_obj_reg, prior_theta_reg, par_names, N, M, data, cnt =0)
```


```{r iteration}
modelName <- "linreg_501_010"
modDir <- set_get_Dir(modelName)$modDir
file <- set_get_Dir(modelName)$file
model_reg = cmdstanr::cmdstan_model(file)
sbc_obj_reg = SBC::SBCModel$new(name="linreg", stan_model=model_reg)
prior_theta_reg = sbc_obj_reg$sample_theta_tilde_stan(par_names, N, data=data)
safe_prior_501_010  <- iter_gen_inf(modelName, sbc_obj_reg, prior_theta_reg, par_names, N, M, data, cnt =0)
```

```{r iteration}
modelName <- "linreg0101"
a = rnorm(1, 0, 1)
b = rnorm(1, 0, 1)
safe_prior_01_01  <- iter_gen_inf(modelName, sbc_obj_reg, prior_theta_reg, par_names, N, M, data, cnt =0)
write.csv(safe_prior_01_01, file = file.path(set_Deliv_Dir(modelName), paste0(paste0(paste0(paste0(modelName, cnt), "_"), medVarRatio), ".csv", sep = "")))
```

## 2. Different datamodel linear regression with unknown mean and variance
```{r iteration}
modelName <- "linreg_unkVar_1020520"
modDir <- set_Model_Dir(modelName)$modDir
model_reg = cmdstanr::cmdstan_model(paste0(modDir, "/linreg/linreg.stan"))
sbc_obj_reg = SBC::SBCModel$new(name="linreg", stan_model=model_reg)
par_names <- list("alpha", "beta", "sigma")
data <- genFakeData_linreg()
prior_theta_reg = sbc_obj_reg$sample_theta_tilde_stan(par_names, N, data=data)
SCS_linreg_unkVar  <- iter_gen_inf(modelName, sbc_obj_reg, prior_theta_reg, par_names, N, M, data, cnt =0)
write.csv(SCS_linreg_unkVar, file = file.path(set_Deliv_Dir(modelName), paste0(paste0(paste0(paste0(modelName), "_"), medVarRatio), ".csv", sep = "")))
```

```{r iteration}
modelName <- "linreg_unkVar_1020_510"
model_reg = cmdstanr::cmdstan_model(paste0(modelDir, "/linreg/linreg.stan"))
sbc_obj_reg = SBC::SBCModel$new(name="linreg", stan_model=model_reg)
par_names <- list("alpha", "beta", "sigma")
prior_theta_reg = sbc_obj_reg$sample_theta_tilde_stan(par_names, N, data=data)
SCS_linreg_unkVar  <- iter_gen_inf(modelName, sbc_obj_reg, prior_theta_reg, par_names, N, M, data, cnt =0)
write.csv(SCS_linreg_unkVar, file = file.path(set_Deliv_Dir(modelName), paste0(paste0(paste0(paste0(modelName, cnt), "_"), medVarRatio), ".csv", sep = "")))
```


```{r iteration}
modelName <- "linreg_unkVar_2020"
model_reg = cmdstanr::cmdstan_model(paste0(modelDir, "/linreg/linreg.stan"))
sbc_obj_reg = SBC::SBCModel$new(name="linreg", stan_model=model_reg)
par_names <- list("alpha", "beta", "sigma")
prior_theta_reg = sbc_obj_reg$sample_theta_tilde_stan(par_names, N, data=data)
SCS_linreg_unkVar  <- iter_gen_inf(modelName, sbc_obj_reg, prior_theta_reg, par_names, N, M, data, cnt =0)
write.csv(SCS_linreg_unkVar, file = file.path(set_Deliv_Dir(modelName), paste0(paste0(paste0(paste0(modelName, cnt), "_"), medVarRatio), ".csv", sep = "")))
```

Comparing the mean posterior and prior for two parameters alpha and beta. Once we repeat this iteration, 

```{r , eight school}

model_cp = cmdstanr::cmdstan_model("tests/eightschools_cp.stan")
sbc_obj_cp = SBC::SBCModel$new(name="eightschools_cp", stan_model=model_cp)
modelName <- "8s"
par_names <- list("theta", "mu", "tau")
J <- 8
y <- c(28, 8, -3, 7, -1, 1, 18, 12)
sigma <- c(15, 10, 16, 11, 9, 11, 10, 18)
data = list("J"=J, "y"=y, "sigma"=sigma)
prior_theta_cp = sbc_obj_cp$sample_theta_tilde_stan(par_names, N, data=data) # partial pars possible
safe_prior_8s  <- iter_gen_inf(modelName, sbc_obj_cp, prior_theta_cp, par_names, N, M, data, cnt =0)
write.csv(safe_prior_8s, file = file.path(set_Deliv_Dir(modelName), paste0(paste0(paste0(modelName, cnt), "_"), ".csv", sep = "")))

 #unlist(dimnames(prior_theta)[[2]]) #pars = list("theta", "mu", "tau")

N = 5
M = 10

ncp_post<- iter_gen_inf(sbc_obj_ncp, prior_theta_ncp, par_names, N, M, data)

```

3. Different computation algorithm
```{r ADVI}
model_code <- "
data {
  int N;
  vector[N] y;
  int K;
  int<lower=1, upper=K> groups[N];
}
parameters {
  vector[K] z;
  real<lower=0> tau;
  real<lower=0> sigma;
}
transformed parameters {
  vector[K] group_mus = z * tau;
}
model {
  y ~ normal(group_mus[groups], sigma);
  sigma ~ normal(0, 1);
  tau ~ normal(0, 1);
  z ~ normal(0, 1);
  sum(group_mus) ~ normal(0, 0.01 * K);
}
"
m <- cmdstan_model(write_stan_file(model_code))

set.seed(8963275)

N <- 40
K <- 10
groups <- rep(1:K, length.out = N)

mu <- sort(rnorm(2, 0, 5))
sigma <- abs(rnorm(1, 0, 1))
tau <- abs(rnorm(1, 0, 1))
group_mus <- rnorm(K, sd = tau)
group_mus <- group_mus - sum(group_mus) / K

cat("True group_mus: ", paste0(group_mus, collapse = "; "), ", tau: ", tau, ", sigma: ", sigma, "\n")

y <- rnorm(N, mean = group_mus[groups], sd = sigma)

dd <- list(N = N, y = y, K = K, groups = groups)

res_sampling <- m$sample(data = dd, refresh = 0, parallel_chains = 4, adapt_delta = 0.95)
res_advi <- m$variational(data = dd)


res_sampling$summary()[,c("variable", "q5", "q95", "ess_bulk")]
res_advi$summary()[,c("variable", "q5", "q95")]
```


4. Decision gradient
```{r well switching}
fit_1 <- stan_glm(switch ~ dist, family=binomial(link="logit"), data=wells)
jitter_binary <- function(a, jitt=0.05){
  ifelse(a==0, runif(length(a), 0, jitt), runif(length(a), 1 - jitt, 1))
}
wells$switch_jitter <- jitter_binary(wells$switch)
plot(wells$dist, wells$switch_jitter)
curve(invlogit(coef(fit_1)[1] + coef(fit_1)[2]*x), add=TRUE)
plot(wells$dist, wells$switch_jitter, xlim=c(0,max(wells$dist)))
curve(invlogit(cbind(1, x/100, 0.5) %*% coef(fit_3)), add=TRUE) 
curve(invlogit(cbind(1, x/100, 1.0) %*% coef(fit_3)), add=TRUE)
wells$dist100 <- wells$dist/100
fit_3 <- stan_glm(switch ~ dist100 + arsenic, family=binomial(link="logit"), data=wells)
curve(invlogit(cbind(1, x/100, 0.5) %*% coef(fit_3)), add=TRUE)
curve(invlogit(cbind(1, x/100, 1.0) %*% coef(fit_3)), add=TRUE)

```
