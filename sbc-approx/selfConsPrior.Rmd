---
title: "SelfConsistentPrior"
output: html_document
---

```{r setup, include=FALSE}
# install.packages("remotes")
remotes::install_github("stan-dev/cmdstanr")
library(cmdstanr)
library(rstan)
library(dplyr)
library(tidyverse)
library(reshape)
library(parallel)
library(bayesplot)
library(posterior)
devtools::install_github("hyunjimoon/SBC")
library(SBC)
set.seed(1954)
.libPaths("~/Rlib")
source("tools/selfCalib.R")
source("tools/functions.r")
options(mc.cores = parallel::detectCores())
library(future)
plan(multisession)
options(SBC.min_chunk_size = 5)
```

# 1. normal model with mean parameter with distribution
```{R}
tv = c("theta") # target variable that determines iteration termination condition
modelName = "simple_normal"
modDir <- set_get_Dir(modelName)$modDir
delivDir <- set_get_Dir(modelName)$delivDir
file <- set_get_Dir(modelName)$file
mod_simple_normal = cmdstanr::cmdstan_model(file)
S = 30 # the number of generated parameter values for each dataset
N = 8 # the number of generated data values for each dataset
M = 1000 # the number of posterior samples for each dataset (nchains * ndraws)

generator <- function(hyperparams, paramvals, predictor = NULL){
  # hyperparamter
  N = hyperparams$N
  S <- dim(paramvals[[1]])
  theta_loc <- rfun(rnorm, ndraws = 1) (S, rvar_mean(paramvals$theta), 0) #rvar<1>[S]
  theta_scale <- rfun(rnorm, ndraws = 1) (S,rvar_sd(paramvals$theta), 0)
  # parameter
  theta <- rfun(rnorm, ndraws = 1)(n = S, mean = theta_loc, sd = theta_scale) 
  # predictor
  # response
  y <- rfun(rnorm, ndraws = N)(n = S, mean = paramvals$theta, sd = 1) 
  gen_rvars <- draws_rvars(y = y, theta_loc = theta_loc, theta_scale = theta_scale, D = N)
  SBC_datasets(
    parameters = as_draws_matrix(paramvals),
    generated = draws_rvars_to_standata(gen_rvars)
  )
}
datasets <- generator(
    hyperparams = list(N = N),
  	paramvals = draws_rvars(theta = rfun(rnorm, ndraws = 1) (n = S, mean = 0, sd = 1))
)
backend <- SBC_backend_cmdstaM(mod_simple_normal, iter_sampling = M)
result <- compute_results(datasets, backend)
paramvals_init <- as_draws_rvars(subset_draws(datasets$parameters, variable = tv))

#target calibration
evolve_df <- list()
evolve_df[[tv]] <- list(median = rep(NA,1), sd = rep(NA,1))
predictor = NULL 
paramvals_sc <- selfCalib(generator, hyperparams, paramvals_init, predictor, backend, tv, cnt = 1, evolve_df, delivDir)
# test self-consistency for paramvals_sc
datasets_sc <- generator(
    hyperparams = list(N = 10),
  	paramvals = paramvals_sc
)
result_sc <- compute_results(datasets_sc, backend) 
# before after compare with ecdf and rank summary
# graphical inspection
g <- plot_ecdf_diff(result)
g_sc <- plot_ecdf_diff(result_sc)
ggsave(g, file =  file.path(delivDir, paste0(paste0(paste0(tv, "_"), "bfCalib_ecdf.png"), sep = "")))
ggsave(g_sc, file =  file.path(delivDir, paste0(paste0(paste0(tv, "_"), "afCalib_ecdf.png"), sep = "")))
# numeric inspection
next_paramvals_init <- adj_post(paramvals_init, result, tv, sumtype = "MtoM") 
next_paramvals_sc <- adj_post(paramvals_sc, result_sc, tv, sumtype = "MtoM")
sc_msr_bfcalib <- set2set(paramvals_init, next_paramvals_init, "theta")                                      
sc_msr_afcalib <- set2set(paramvals_sc, next_paramvals_sc, "theta")
csv_save(list(before = unlist(sc_msr_bfcalib), after = unlist(sc_msr_afcalib)), "theta", delivDir, type = "diagnositcs")
```
Both numerical (cumulative JS distance: 0.17 vs 0.099) and graphical ecdf indicate that `sc_g` is better calibrated than `g`.

## 2. sgmd model
### 2.1.sgmd model1 with different data region; x = rnorm(20, [0, 5], 2) 
With zero mean for x, datasets from all `prior_width` are good. Generator input consists of true parameter point values `ture_priorvals` and `predictor`. Note the difference in plural and singular. Compared to parameters which has variability between each prior values (rvar<S>[1], e.g.0.026 ± 0.99), predictor's variability exist only within one datasets. All dataset from each priorpoint have the same value of predictor (rvar<1>[S]), e.g. 1.4724 ± NA).
```{R sgmd1 with 1/(1+exp^(-wx + b))}
modelName = "sigmoid"
delivDir <- set_get_Dir(modelName)$delivDir
file <- set_get_Dir(modelName)$file
mod_sgmd = cmdstanr::cmdstan_model(file)
S = 200
N = 1
M = 1000
prior_width = 10
thin = 4
generator_sgmd <- function(hyperparams, paramvals, predictor){
  # hyperparamter
  N = hyperparams$N
  sigma = hyperparams$sigma
  prior_width = hyperparams$prior_width
  # paramter
  w = paramvals$w #S vector
  b = paramvals$b
  # predictor
  x = predictor$x 
  # generate
  ndraws(paramvals[[1]])
  y_true = 1 / (1 + exp(-w*x-b))
  y <- rfun(rnorm, ndraws = N)(n = S, mean = y_true, sd = sigma) 
  gen_rvars <- draws_rvars(N = S, x = x, y = y, prior_width = prior_width, sigma = sigma)
  SBC_datasets(
    parameters = as_draws_matrix(paramvals), 
    generated = draws_rvars_to_standata(gen_rvars)
  )
}
backend_sgmd <- SBC_backend_cmdstan_sample(mod_sgmd)
datasets_sgmd_x0 <- generator_sgmd(
	paramvals = draws_rvars(w = rvar(rnorm(S, 0, 1)), b = rvar(rnorm(S, 0, 1))), 
	predictor = draws_rvars(x = rfun(rnorm, ndraws = 1) (n = S, 0, 2)), 
	hyperparams = list(prior_width = 10, sigma = 0.1, N = 10),
	)
result_x0 <- compute_results(datasets_sgmd_x0, backend_sgmd)
plot_rank_hist(result_x0)

fit_sgmd_divergent_rstan <- sampling(rstan::stan_model(file = set_get_Dir(modelName)$file), data = datasets_sgmd_x0$generated[[1]])
pairs(fit_sgmd_divergent_rstan, pars = "energy__", include = FALSE)
```

Different data region with the same likelihood and prior distribution can be divergent. Although the calibration was good with $x \sim N(0,2^2)$ and without any divergence, both SBC ecdf_diff and pairs plot returns bad result for the dataspace corresponding to predictor $x \sim norm(5,2)$ with the same likelihood and prior distribution.
```{R, sgmd1 with norm(5,2) dataset}
set.seed(2000)
datasets_sgmd_x5_init <- generator_sgmd(
  hyperparams = list(prior_width = 10, sigma = 0.1, N = 10),
	paramvals = draws_rvars(w = rvar(rnorm(S, 0, 1)), b = rvar(rnorm(S, 0, 1))), 
	predictor = draws_rvars(x = rfun(rnorm, ndraws = 1) (n = S, 5, 2))
	)
result_sgmd_x5_init <- compute_results(datasets_sgmd_x5_init, backend_sgmd)
plot_ecdf_diff(result_sgmd_x5_init)
fit_sgmd_divergent <- sampling(rstan::stan_model(file = set_get_Dir(modelName)$file),
                                        data = datasets_sgmd_x5_init$generated[[1]])
pairs(fit_sgmd_divergent, pars = "energy__", include = FALSE)
```

Would changing the `prior width` mitigate the problem? From 10 to 1 or 30? It turns out wider prior width leads to greater divergence.
```{R}
set.seed(1954)
datasets_sgmd_x3_w30 <-  generator_sgmd(
	paramvals = draws_rvars(w = rvar(rnorm(S, 0, 1)), b = rvar(rnorm(S, 0, 1))), 
	predictor = draws_rvars(x = rfun(rnorm, ndraws = N) (n = S, 3, 2)), 
	hyperparams = list(prior_width = 1, sigma = 0.1, N = 10)
	)
result_sgmd_x3_w30 <- compute_results(datasets_sgmd_x3_w30, backend_sgmd)
plot_ecdf_diff(result_sgmd_x5_w30)
fit_sgmd_divergent_w30 <- sampling(rstan::stan_model(file = set_get_Dir(modelName)$file), data = datasets_sgmd_x5_w30$generated[[1]])
pairs(fit_sgmd_divergent_w30, pars = "energy__", include = FALSE)
```

Can self-calibration drive the initial prior to a well-calibrated parameter space i.e. auto-calibrated? It seems for this example, the current implementation fo prior
```{R sigmoid auto-calib}
set.seed(2000)
# target calibration
tv = c("w","b")
priors_sgmd_x5_init <- as_draws_rvars(subset_draws(datasets_sgmd_x5_w30$parameters, variable = tv))
evolve_df <- list()
for (v in tv){evolve_df[[v]] <- list(median = rep(NA,1), sd = rep(NA,1))}
generator_sgmd(
	paramvals = draws_rvars(w = rvar(rnorm(S, 0, 1)), b = rvar(rnorm(S, 0, 1))), 
	predictor = draws_rvars(x = rfun(rnorm, ndraws = 1) (n = S, 5, 2)), 
	hyperparams = list(prior_width = 10, sigma = 0.1, N = 10)
	)

predictor <- draws_rvars(x = rfun(rnorm, ndraws = 1) (n = S, 5, 2))
#inerator, hyperparam, param, predictor, backend, target_vars, cnt, thin, evolve_df, delivDir
priors_sgmd_x5_sc <- selfCalib(generator_sgmd, hyperparams, priors_sgmd_x5_init, predictor, backend_sgmd, tv, cnt = 1,thin, evolve_df, delivDir)
datasets_sgmd_x5_sc <- generator_priorvals_sgmd(priors_sgmd_x5_sc, N, predictor) 
result_sgmd_x5_sc <- compute_results(datasets_sgmd_x5_sc, backend_sgmd)

# before after compare with graphical and numerical summary
g <- plot_ecdf_diff(result_sgmd_x5_init)
sc_g <- plot_ecdf_diff(result_sgmd_x5_sc)
ggsave(g, file =  file.path(delivDir, "bfCalib_ecdf.png"))
ggsave(sc_g, file = file.path(delivDir, "afCalib_ecdf.png"))

# numeric inspection
priors_next <- adj_post(priors_sgmd_x5_init, result_sgmd_x5_sc, tv, sumtype = "Mto1_randpick") 
priors_next_sc <- adj_post(priors_sgmd_x5_sc, result_sgmd_x5_sc, tv, sumtype = "Mto1_randpick")
self_cons_msr_bfcalib <- list()
self_cons_msr_afcalib <- list()
self_cons_msr_bfcalib["w"] <- set2set(priors_sgmd_x5_init, priors_next, "w")
self_cons_msr_bfcalib["b"] <- set2set(priors_sgmd_x5_init, priors_next, "b")
self_cons_msr_afcalib["w"] <- set2set(priors_sgmd_x5_sc, priors_next_sc, "w")
self_cons_msr_afcalib["b"] <- set2set(priors_sgmd_x5_sc, priors_next_sc, "b")
csv_save(list(before = self_cons_msr_bfcalib, after = self_cons_msr_afcalib), delivDir, type = "diagnositcs")
```
